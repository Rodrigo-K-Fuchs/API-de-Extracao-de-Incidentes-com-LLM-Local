# ğŸ§  API de ExtraÃ§Ã£o de Incidentes com LLM Local (Ollama)

API que recebe textos livres descrevendo incidentes e retorna informaÃ§Ãµes estruturadas em JSON, utilizando um LLM local via Ollama.

---

## ğŸ¯ Objetivo

Demonstrar um pipeline completo de extraÃ§Ã£o de informaÃ§Ãµes com:

- PrÃ©-processamento determinÃ­stico de texto
- ExtraÃ§Ã£o semÃ¢ntica com LLM local
- ValidaÃ§Ã£o estrutural com Pydantic
- API HTTP simples e documentada

Tudo isso sem depender de serviÃ§os externos.

---

## ğŸ—ï¸ Arquitetura

```
UsuÃ¡rio â†’ FastAPI â†’ PrÃ©-processamento â†’ Ollama (LLM) â†’ Pydantic â†’ JSON estruturado
```

1. UsuÃ¡rio envia um texto via API
2. O texto passa por prÃ©-processamento determinÃ­stico
3. O texto tratado Ã© enviado ao LLM local (Ollama)
4. O retorno do LLM Ã© validado com Pydantic
5. A API devolve um JSON estruturado

---

## ğŸ§¹ PrÃ©-Processamento de Texto

Antes de qualquer chamada ao LLM, o texto passa por regras fixas, previsÃ­veis e testÃ¡veis:

- **NormalizaÃ§Ã£o** â€” lowercase e limpeza geral
- **RemoÃ§Ã£o de acentos**
- **PadronizaÃ§Ã£o de datas e horas**
- **ExtraÃ§Ã£o de hints temporais**
- **Fuzzy matching** com distÃ¢ncia de Levenshtein â€” corrige pequenas variaÃ§Ãµes de palavras e reduz dependÃªncia do LLM

Isso garante maior **consistÃªncia**, **reprodutibilidade** e **testabilidade** no pipeline.

---

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ api.py                        # Ponto de entrada da API FastAPI
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ incident_extractor.py     # Orquestra o pipeline (prompt + LLM + parsing)
â”‚   â””â”€â”€ text_preprocessor.py     # PrÃ©-processamento determinÃ­stico do texto
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ incident.py               # Modelo Pydantic do incidente
â”‚   â””â”€â”€ incident_prompt.py       # Prompt utilizado pelo LLM
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                     # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ integration/              # Testes de integraÃ§Ã£o
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Requisitos

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Ollama](https://ollama.com) instalado localmente

> **AtenÃ§Ã£o:** o Ollama nÃ£o roda dentro do container. A API apenas se comunica com ele via HTTP.

---

## ğŸ¤– Instalando o Ollama

ApÃ³s instalar o Ollama, baixe o modelo e inicie o servidor:

```bash
ollama pull llama3.2
ollama serve
```

---

## ğŸ³ Rodando com Docker

### 1. Build da imagem

```bash
docker build -t incident-api .
```

### 2. Subindo a aplicaÃ§Ã£o

**OpÃ§Ã£o A â€” Docker Compose (recomendado)**

```bash
docker compose up
```

**OpÃ§Ã£o B â€” Docker Run**

```bash
docker run -p 8000:8000 \
  -e OLLAMA_HOST=http://host.docker.internal:11434 \
  -e OLLAMA_MODEL=llama3.2 \
  incident-api
```

A API estarÃ¡ disponÃ­vel em `http://localhost:8000`.

---

## ğŸŒ DocumentaÃ§Ã£o Interativa

Acesse a interface Swagger para testar a API diretamente no navegador:

```
http://localhost:8000/docs
```

---

## ğŸ§ª Testes

O projeto possui testes unitÃ¡rios (prÃ©-processamento, validaÃ§Ãµes e regras determinÃ­sticas) e testes de integraÃ§Ã£o (pipeline completo com LLM mockado).

```bash
pytest
```

---

## âš ï¸ Regras do Sistema

- Campos nÃ£o inferÃ­veis retornam `null`
- HorÃ¡rios impossÃ­veis retornam `"INVALIDO"`
- Nenhuma informaÃ§Ã£o Ã© inventada pelo LLM
- Todo output passa por validaÃ§Ã£o Pydantic
- O LLM nÃ£o decide sozinho â€” o cÃ³digo manda

---

## ğŸ“– DocumentaÃ§Ã£o no CÃ³digo

Todas as classes principais possuem docstrings descrevendo responsabilidade, entradas, saÃ­das e comportamento esperado.
