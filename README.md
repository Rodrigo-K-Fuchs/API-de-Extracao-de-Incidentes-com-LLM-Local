# üß† API de Extra√ß√£o de Incidentes com LLM Local (Ollama)

API que recebe textos livres descrevendo incidentes e retorna informa√ß√µes estruturadas em JSON, utilizando um LLM local via Ollama.

---

## üéØ Objetivo

Demonstrar um pipeline completo de extra√ß√£o de informa√ß√µes com:

- Pr√©-processamento determin√≠stico de texto
- Extra√ß√£o sem√¢ntica com LLM local
- Valida√ß√£o estrutural com Pydantic
- API HTTP simples e documentada

Tudo isso sem depender de servi√ßos externos.

---

## üèóÔ∏è Arquitetura

```
Usu√°rio ‚Üí FastAPI ‚Üí Pr√©-processamento ‚Üí Ollama (LLM) ‚Üí Pydantic ‚Üí JSON estruturado
```

1. Usu√°rio envia um texto via API
2. O texto passa por pr√©-processamento determin√≠stico
3. O texto tratado √© enviado ao LLM local (Ollama)
4. O retorno do LLM √© validado com Pydantic
5. A API devolve um JSON estruturado

---

## üßπ Pr√©-Processamento de Texto

Antes de qualquer chamada ao LLM, o texto passa por regras fixas, previs√≠veis e test√°veis:

- **Normaliza√ß√£o** ‚Äî lowercase e limpeza geral
- **Remo√ß√£o de acentos**
- **Padroniza√ß√£o de datas e horas**
- **Extra√ß√£o de hints temporais**
- **Fuzzy matching** com dist√¢ncia de Levenshtein ‚Äî corrige pequenas varia√ß√µes de palavras e reduz depend√™ncia do LLM

Isso garante maior **consist√™ncia**, **reprodutibilidade** e **testabilidade** no pipeline.

---

## üìÅ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ api.py                        # Ponto de entrada da API FastAPI
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ incident_extractor.py     # Orquestra o pipeline (prompt + LLM + parsing)
‚îÇ   ‚îî‚îÄ‚îÄ text_preprocessor.py     # Pr√©-processamento determin√≠stico do texto
‚îÇ
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ incident.py               # Modelo Pydantic do incidente
‚îÇ   ‚îî‚îÄ‚îÄ incident_prompt.py        # Prompt utilizado pelo LLM
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                     # Testes unit√°rios
‚îÇ   ‚îú‚îÄ‚îÄ integration/              # Testes de integra√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Pr√©-requisitos

Antes de come√ßar, voc√™ precisa ter instalado:

- [Docker Desktop](https://docs.docker.com/get-started/introduction/get-docker-desktop/)
- [Git](https://git-scm.com/downloads) ou [GitHub CLI](https://cli.github.com/) *(para clonar o reposit√≥rio)*

---

## üöÄ Instala√ß√£o e Configura√ß√£o

### 1. Instalar o Docker Desktop e Git

Acesse e siga as instru√ß√µes do instalador para o seu sistema operacional:

üëâ https://git-scm.com/install/windows
üëâ https://docs.docker.com/get-started/introduction/get-docker-desktop/

Ap√≥s a instala√ß√£o, **abra o Docker Desktop** e aguarde ele inicializar completamente (√≠cone na bandeja do sistema deve ficar verde/est√°vel).

### 2. Clonar o reposit√≥rio

**Op√ß√£o A ‚Äî GitHub CLI (recomendado):**

```bash
gh repo clone Rodrigo-K-Fuchs/API-de-Extracao-de-Incidentes-com-LLM-Local
```

**Op√ß√£o B ‚Äî Git padr√£o:**

```bash
git clone https://github.com/Rodrigo-K-Fuchs/API-de-Extracao-de-Incidentes-com-LLM-Local.git
```

**Op√ß√£o C ‚Äî Download ZIP:**

Na p√°gina do reposit√≥rio no GitHub, clique em `Code` ‚Üí `Download ZIP` e extraia o conte√∫do.

---

### 3. Navegar at√© a pasta do projeto

Ap√≥s clonar ou extrair o ZIP, acesse a pasta raiz do projeto:

```bash
cd API-de-Extracao-de-Incidentes-com-LLM-Local
```

> ‚ö†Ô∏è **Todos os comandos a seguir devem ser executados a partir desta pasta.**

---

## üê≥ Rodando com Docker

### Build da imagem

```bash
docker build -t incident-api .
```

### Subindo a aplica√ß√£o

```bash
docker compose up
```
Para encerrar:
CTRL + C
---

## üåê Documenta√ß√£o Interativa

Com a aplica√ß√£o rodando, acesse a interface Swagger para testar a API diretamente no navegador:

```
http://localhost:8000/docs
```

---

## üß™ Testes

O projeto possui testes unit√°rios (pr√©-processamento, valida√ß√µes e regras determin√≠sticas) e testes de integra√ß√£o (pipeline completo com LLM mockado).

Para executar os testes, abra o projeto em uma IDE ou editor de texto e, no terminal na raiz do projeto, execute:

```bash
pytest
```

> Os testes de integra√ß√£o utilizam mock do LLM e **n√£o requerem** o Ollama rodando.

---

## ‚ö†Ô∏è Regras do Sistema

- Campos n√£o infer√≠veis retornam `null`
- Hor√°rios imposs√≠veis retornam `"INVALIDO"`
- Nenhuma informa√ß√£o √© inventada pelo LLM
- Todo output passa por valida√ß√£o Pydantic
- O LLM n√£o decide sozinho ‚Äî o c√≥digo manda

---

## üìñ Documenta√ß√£o no C√≥digo

Todas as classes principais possuem docstrings descrevendo responsabilidade, entradas, sa√≠das e comportamento esperado.
